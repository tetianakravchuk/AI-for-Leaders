{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5190b54c-d49c-4d6f-80cc-22555336a9cd",
   "metadata": {},
   "source": [
    "# Week 2 - Preprocessing, part 2\n",
    "\n",
    "# 1. Lesson: None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c4e5ff-b05f-4ef2-96f1-49dcb5beb158",
   "metadata": {},
   "source": [
    "# 2. Weekly graph question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad37e29-6e84-41fa-886d-abc1312213ab",
   "metadata": {},
   "source": [
    "The Storytelling With Data book mentions planning on a \"Who, What, and How\" for your data story.  Write down a possible Who, What, and How for your data, using the ideas in the book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11ca46b",
   "metadata": {},
   "source": [
    "**Who:** Identify your audience specifically. For example, if you’re presenting to a budget committee, your audience might be the decision-makers who can approve funding for a project. Knowing your audience helps tailor your message to their needs and interests.\n",
    "\n",
    "**What:** Clearly define what you want your audience to know or do. For instance, they should recognize the success of a pilot program and approve funding to continue it. Articulate this message concisely to ensure it resonates with your audience.\n",
    "\n",
    "**How:** Determine the best way to communicate your message. This could involve using a presentation with visuals to highlight key data points or a detailed report for them to review at their own pace. Consider the level of detail required and the tone you want to set to ensure effective communication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898eb327-aefd-4ac0-b95a-92b616a2181b",
   "metadata": {},
   "source": [
    "# 3. Homework - work with your own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe925521-979f-4983-8d85-8db8d1316e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14836788-b235-4cd4-b94d-5f749c6141a8",
   "metadata": {},
   "source": [
    "This week, you will do the same types of exercises as last week, but you should use your own datasets that you found last semester.\n",
    "\n",
    "### Here are some types of analysis you can do  Use Google, documentation, and ChatGPT to help you:\n",
    "\n",
    "- Summarize the datasets using info() and describe()\n",
    "\n",
    "- Are there any duplicate rows?\n",
    "\n",
    "- Are there any duplicate values in a given column (when this would be inappropriate?)\n",
    "\n",
    "- What are the mean, median, and mode of each column?\n",
    "\n",
    "- Are there any missing or null values?\n",
    "\n",
    "    - Do you want to fill in the missing value with a mean value?  A value of your choice?  Remove that row?\n",
    "\n",
    "- Identify any other inconsistent data (e.g. someone seems to be taking an action before they are born.)\n",
    "\n",
    "- Encode any categorical variables (e.g. with one-hot encoding.)\n",
    "\n",
    "### Conclusions:\n",
    "\n",
    "- Are the data usable?  If not, find some new data!\n",
    "\n",
    "- Do you need to modify or correct the data in some way?\n",
    "\n",
    "- Is there any class imbalance?  (Categories that have many more items than other categories)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42d57c9",
   "metadata": {},
   "source": [
    "a. Using info():\n",
    "The info() method provides a concise summary of the DataFrame, including the number of non-null entries and data types for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f93bdf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 541909 entries, 0 to 541908\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   InvoiceNo    541909 non-null  object \n",
      " 1   StockCode    541909 non-null  object \n",
      " 2   Description  540455 non-null  object \n",
      " 3   Quantity     541909 non-null  int64  \n",
      " 4   InvoiceDate  541909 non-null  object \n",
      " 5   UnitPrice    541909 non-null  float64\n",
      " 6   CustomerID   406829 non-null  float64\n",
      " 7   Country      541909 non-null  object \n",
      "dtypes: float64(2), int64(1), object(5)\n",
      "memory usage: 33.1+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('e-commerce.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Display summary information\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21965301",
   "metadata": {},
   "source": [
    "b. Using describe():\n",
    "The describe() method offers descriptive statistics for numerical columns, such as mean, standard deviation, and percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3713e71c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Quantity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "UnitPrice",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "CustomerID",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "da8e32cc-694e-467d-9f16-93f0d8042965",
       "rows": [
        [
         "count",
         "541909.0",
         "541909.0",
         "406829.0"
        ],
        [
         "mean",
         "9.55224954743324",
         "4.611113626088513",
         "15287.690570239585"
        ],
        [
         "std",
         "218.08115784986612",
         "96.75985306119716",
         "1713.6003033216632"
        ],
        [
         "min",
         "-80995.0",
         "-11062.06",
         "12346.0"
        ],
        [
         "25%",
         "1.0",
         "1.25",
         "13953.0"
        ],
        [
         "50%",
         "3.0",
         "2.08",
         "15152.0"
        ],
        [
         "75%",
         "10.0",
         "4.13",
         "16791.0"
        ],
        [
         "max",
         "80995.0",
         "38970.0",
         "18287.0"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quantity</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>CustomerID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>541909.000000</td>\n",
       "      <td>541909.000000</td>\n",
       "      <td>406829.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.552250</td>\n",
       "      <td>4.611114</td>\n",
       "      <td>15287.690570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>218.081158</td>\n",
       "      <td>96.759853</td>\n",
       "      <td>1713.600303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-80995.000000</td>\n",
       "      <td>-11062.060000</td>\n",
       "      <td>12346.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>13953.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.080000</td>\n",
       "      <td>15152.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.130000</td>\n",
       "      <td>16791.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>80995.000000</td>\n",
       "      <td>38970.000000</td>\n",
       "      <td>18287.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Quantity      UnitPrice     CustomerID\n",
       "count  541909.000000  541909.000000  406829.000000\n",
       "mean        9.552250       4.611114   15287.690570\n",
       "std       218.081158      96.759853    1713.600303\n",
       "min    -80995.000000  -11062.060000   12346.000000\n",
       "25%         1.000000       1.250000   13953.000000\n",
       "50%         3.000000       2.080000   15152.000000\n",
       "75%        10.000000       4.130000   16791.000000\n",
       "max     80995.000000   38970.000000   18287.000000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display descriptive statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8831df",
   "metadata": {},
   "source": [
    "2. Checking for Duplicate Rows\n",
    "\n",
    "Duplicate rows can skew analysis results. To identify and handle them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d081de07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 5268\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate rows\n",
    "duplicate_rows = df[df.duplicated()]\n",
    "print(f\"Number of duplicate rows: {duplicate_rows.shape[0]}\")\n",
    "\n",
    "# Optionally, remove duplicate rows\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eee91a",
   "metadata": {},
   "source": [
    "3. Checking for Duplicate Values in Specific Columns\n",
    "\n",
    "Certain columns, like InvoiceNo (assuming it represents unique transactions), should not have duplicate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb81fad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate InvoiceNo entries: 510741\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate InvoiceNo\n",
    "duplicate_invoices = df[df.duplicated(subset=['InvoiceNo'])]\n",
    "print(f\"Number of duplicate InvoiceNo entries: {duplicate_invoices.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e9e814",
   "metadata": {},
   "source": [
    "4. Calculating Mean, Median, and Mode\n",
    "\n",
    "Understanding central tendencies helps in grasping the data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9089730b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Calculate mean, median, and mode for numerical columns\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m mean_values \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m      3\u001b[0m median_values \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mmedian()\n\u001b[1;32m      4\u001b[0m mode_values \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mmode()\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# mode() returns a DataFrame\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:11693\u001b[0m, in \u001b[0;36mDataFrame.mean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  11685\u001b[0m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m  11686\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[1;32m  11687\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  11691\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m  11692\u001b[0m ):\n\u001b[0;32m> 11693\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mmean(axis, skipna, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m  11694\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, Series):\n\u001b[1;32m  11695\u001b[0m         result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py:12420\u001b[0m, in \u001b[0;36mNDFrame.mean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[1;32m  12414\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  12415\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  12418\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m  12419\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m> 12420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stat_function(\n\u001b[1;32m  12421\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, nanops\u001b[38;5;241m.\u001b[39mnanmean, axis, skipna, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m  12422\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py:12377\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[0;34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12373\u001b[0m nv\u001b[38;5;241m.\u001b[39mvalidate_func(name, (), kwargs)\n\u001b[1;32m  12375\u001b[0m validate_bool_kwarg(skipna, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskipna\u001b[39m\u001b[38;5;124m\"\u001b[39m, none_allowed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m> 12377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reduce(\n\u001b[1;32m  12378\u001b[0m     func, name\u001b[38;5;241m=\u001b[39mname, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only\n\u001b[1;32m  12379\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:11562\u001b[0m, in \u001b[0;36mDataFrame._reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m  11558\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m  11560\u001b[0m \u001b[38;5;66;03m# After possibly _get_data and transposing, we are now in the\u001b[39;00m\n\u001b[1;32m  11561\u001b[0m \u001b[38;5;66;03m#  simple case where we can use BlockManager.reduce\u001b[39;00m\n\u001b[0;32m> 11562\u001b[0m res \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mreduce(blk_func)\n\u001b[1;32m  11563\u001b[0m out \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(res, axes\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m  11564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboolean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/internals/managers.py:1500\u001b[0m, in \u001b[0;36mBlockManager.reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m   1498\u001b[0m res_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m-> 1500\u001b[0m     nbs \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mreduce(func)\n\u001b[1;32m   1501\u001b[0m     res_blocks\u001b[38;5;241m.\u001b[39mextend(nbs)\n\u001b[1;32m   1503\u001b[0m index \u001b[38;5;241m=\u001b[39m Index([\u001b[38;5;28;01mNone\u001b[39;00m])  \u001b[38;5;66;03m# placeholder\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/internals/blocks.py:404\u001b[0m, in \u001b[0;36mBlock.reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreduce\u001b[39m(\u001b[38;5;28mself\u001b[39m, func) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;66;03m# We will apply the function and reshape the result into a single-row\u001b[39;00m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m#  Block with the same mgr_locs; squeezing will be done at a higher level\u001b[39;00m\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 404\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    407\u001b[0m         res_values \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:11481\u001b[0m, in \u001b[0;36mDataFrame._reduce.<locals>.blk_func\u001b[0;34m(values, axis)\u001b[0m\n\u001b[1;32m  11479\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([result])\n\u001b[1;32m  11480\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m> 11481\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/nanops.py:147\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    145\u001b[0m         result \u001b[38;5;241m=\u001b[39m alt(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     result \u001b[38;5;241m=\u001b[39m alt(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/nanops.py:404\u001b[0m, in \u001b[0;36m_datetimelike_compat.<locals>.new_func\u001b[0;34m(values, axis, skipna, mask, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike \u001b[38;5;129;01mand\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     mask \u001b[38;5;241m=\u001b[39m isna(values)\n\u001b[0;32m--> 404\u001b[0m result \u001b[38;5;241m=\u001b[39m func(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, mask\u001b[38;5;241m=\u001b[39mmask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike:\n\u001b[1;32m    407\u001b[0m     result \u001b[38;5;241m=\u001b[39m _wrap_results(result, orig_values\u001b[38;5;241m.\u001b[39mdtype, fill_value\u001b[38;5;241m=\u001b[39miNaT)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/nanops.py:719\u001b[0m, in \u001b[0;36mnanmean\u001b[0;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[1;32m    716\u001b[0m     dtype_count \u001b[38;5;241m=\u001b[39m dtype\n\u001b[1;32m    718\u001b[0m count \u001b[38;5;241m=\u001b[39m _get_counts(values\u001b[38;5;241m.\u001b[39mshape, mask, axis, dtype\u001b[38;5;241m=\u001b[39mdtype_count)\n\u001b[0;32m--> 719\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39msum(axis, dtype\u001b[38;5;241m=\u001b[39mdtype_sum)\n\u001b[1;32m    720\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m _ensure_numeric(the_sum)\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(the_sum, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/numpy/core/_methods.py:49\u001b[0m, in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     48\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "source": [
    "# Calculate mean, median, and mode for numerical columns\n",
    "mean_values = df.mean()\n",
    "median_values = df.median()\n",
    "mode_values = df.mode().iloc[0]  # mode() returns a DataFrame\n",
    "\n",
    "print(\"Mean values:\\n\", mean_values)\n",
    "print(\"\\nMedian values:\\n\", median_values)\n",
    "print(\"\\nMode values:\\n\", mode_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb93ed8",
   "metadata": {},
   "source": [
    "5. Identifying Missing or Null Values\n",
    "\n",
    "Missing values can affect analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ab88e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b167b7",
   "metadata": {},
   "source": [
    "Filling with Mean: Suitable for numerical columns where the mean is a representative value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310f1e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in 'UnitPrice' with the mean\n",
    "df['UnitPrice'].fillna(df['UnitPrice'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea856ec",
   "metadata": {},
   "source": [
    "Filling with a Specific Value: Useful for categorical columns or when a default value is appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8de19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in 'Country' with 'Unknown'\n",
    "df['Country'].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e2188b",
   "metadata": {},
   "source": [
    "Removing Rows: Consider when the missing data is minimal or irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902c2b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with any missing values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f9912e",
   "metadata": {},
   "source": [
    "\tRemoving Rows: Consider when the missing data is minimal or irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51329043",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbdbcb6",
   "metadata": {},
   "source": [
    "6. Identifying Inconsistent Data\n",
    "\n",
    "Inconsistent data, such as negative quantities or future dates in past records, can lead to incorrect insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a152f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for negative quantities\n",
    "negative_quantities = df[df['Quantity'] < 0]\n",
    "print(f\"Number of records with negative quantities: {negative_quantities.shape[0]}\")\n",
    "\n",
    "# Check for future dates (assuming 'InvoiceDate' is a datetime column)\n",
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], errors='coerce')\n",
    "future_dates = df[df['InvoiceDate'] > pd.Timestamp.now()]\n",
    "print(f\"Number of records with future InvoiceDate: {future_dates.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd42b8e",
   "metadata": {},
   "source": [
    "7. Encoding Categorical Variables\n",
    "\n",
    "Machine learning models require numerical input, so encoding categorical variables is essential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f32c2ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the 'Country' column\n",
    "df_encoded = pd.get_dummies(df, columns=['Country'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45223e41",
   "metadata": {},
   "source": [
    "🏁 Conclusions\n",
    "\n",
    "✅ Are the data usable?\n",
    "\n",
    "Yes, the E-Commerce dataset is generally usable, but it requires some preprocessing and cleaning to ensure accuracy in analysis.\n",
    "\n",
    "Key issues found:\n",
    "\t•\tDuplicate rows were detected, which need to be removed.\n",
    "\t•\tMissing values were present in certain columns, requiring imputation or removal.\n",
    "\t•\tInconsistent data (e.g., negative quantities, future invoice dates) need correction.\n",
    "\n",
    "If these issues significantly impact analysis, finding an alternative dataset may be necessary.\n",
    "\n",
    "🔄 Do you need to modify or correct the data in some way?\n",
    "\n",
    "Yes, data modifications and corrections are required for optimal usability:\n",
    "\t•\tHandling duplicates: Removing duplicate rows to prevent skewed results.\n",
    "\t•\tFilling missing values: Using appropriate strategies such as mean imputation for numerical data and a default value for categorical fields.\n",
    "\t•\tCorrecting inconsistent values:\n",
    "\t•\tRemoving negative Quantity values unless they represent refunds.\n",
    "\t•\tChecking for future InvoiceDate entries and filtering them out if incorrect.\n",
    "\t•\tEncoding categorical variables: Converting Country into numerical format using one-hot encoding for modeling.\n",
    "\n",
    "⚖ Is there any class imbalance?\n",
    "\n",
    "Class imbalance analysis involves checking if certain categories are overrepresented compared to others.\n",
    "\n",
    "Findings:\n",
    "\t•\tThe country distribution is highly skewed, with most transactions occurring in the UK compared to other countries.\n",
    "\t•\tSome products are sold significantly more often than others, creating an imbalance in sales data.\n",
    "\t•\tSome customers may place many more orders than others, affecting behavioral analysis.\n",
    "\n",
    "Potential impact:\n",
    "\t•\tIf using machine learning, imbalanced data can lead to biased models.\n",
    "\t•\tIf performing statistical analysis, major categories may dominate insights.\n",
    "\n",
    "Possible solutions:\n",
    "\t•\tDownsampling: Reducing the number of samples from overrepresented categories.\n",
    "\t•\tUpsampling: Adding synthetic data for underrepresented groups.\n",
    "\t•\tWeighted metrics: Adjusting evaluation metrics to balance performance.\n",
    "\n",
    "🚀 Final Thoughts\n",
    "\n",
    "This dataset is usable with proper preprocessing, and the analysis is viable after handling missing data, duplicates, and inconsistencies.\n",
    "\n",
    "However, class imbalance and data modifications must be carefully addressed depending on the goal of the analysis. If severe issues remain after cleaning, consider finding a better dataset with more balanced and complete information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abab9e6d-18cc-4863-b980-3e52f581763a",
   "metadata": {},
   "source": [
    "# 4. Storytelling With Data graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1911148d-9df6-4b33-a875-8c96408ec834",
   "metadata": {},
   "source": [
    "Just like last week: choose any graph in the Introduction of Storytelling With Data. Use matplotlib to reproduce it in a rough way. I don't expect you to spend an enormous amount of time on this; I understand that you likely will not have time to re-create every feature of the graph. However, if you're excited about learning to use matplotlib, this is a good way to do that. You don't have to duplicate the exact values on the graph; just the same rough shape will be enough.  If you don't feel comfortable using matplotlib yet, do the best you can and write down what you tried or what Google searches you did to find the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2888f9-3700-45ab-9829-6a5372106f9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
